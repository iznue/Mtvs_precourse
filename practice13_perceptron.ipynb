{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iznue/Mtvs/blob/main/practice13_perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Learning with PyTorch \n",
        "### 실습04. Perceptron Modeling : AND Function\n",
        "\n",
        "1. Prepare Dataset \n",
        "2. Train Model\n",
        "3. Test Model\n",
        "\n",
        "  \n",
        "<img src=\"https://dl.dropbox.com/s/enqqtqnegzb6ceg/and.png\" width=\"400\"/>"
      ],
      "metadata": {
        "id": "XiIqfqju75-J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Prepare Data"
      ],
      "metadata": {
        "id": "mF786Lnk8IQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1-1. 데이터 준비\n",
        "\n",
        "# data : tensor type\n",
        "\n",
        "import torch\n",
        "\n",
        "X = torch.FloatTensor([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = torch.FloatTensor([[0],[0],[0],[1]])\n",
        "\n",
        "print(X)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "xj2ZuvzW8G-M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ac58b7b-a7a8-4ae3-f763-0f684781e5e0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [1., 1.]])\n",
            "tensor([[0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1-2. 데이터 확인\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "Ir9DSKL9qqR3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d6ca938-019f-4738-8646-39308aa083e3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 2])\n",
            "torch.Size([4, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Train Model"
      ],
      "metadata": {
        "id": "sH8LJ5O6mgly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Design Network : 어떤 레이어, 몇층의 레이어, 각 레이어 몇 노드\n",
        "* Loss Func & Optimizer\n",
        "* Train Loop (Forward, back, update)"
      ],
      "metadata": {
        "id": "t2W64CCyxWp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2-1. Network\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "linear = nn.Linear(2, 1) # 데이터의 개수가 아니라 하나의 샘플에 들어있는 feature\n",
        "# 데이터 개수가 아닌 하나의 샘플에 들어있는 feature 입력\n",
        "# 모든 output은 activation layer를 가짐 -> 활성함수 : sigmoid\n",
        "# sigmoid를 미분했을 때 도함수의 최댓값은 0.2를 조금 넘음\n",
        "# 역전파 시 chain-rule을 이용해 미분이 여러번 일어나게 되는데\n",
        "# 이때 sigmoid를 미분한 값이 너무 작아서 gradient가 뒤로 갈수록 사라지는 vanishing gradient가 발생함\n",
        "# 따라서 최근은 RuLU함수를 이용함 (은닉층)\n",
        "sigmoid = nn.Sigmoid() # activation func\n",
        "\n",
        "# 순차적 계층으로 모델 정의\n",
        "model = nn.Sequential(linear,\n",
        "                      sigmoid)\n",
        "# 순차적으로 가지고있는 module(layer, activation function)을 쌓아줌\n",
        "print(model)"
      ],
      "metadata": {
        "id": "q2rKRqIr9sRC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "561eb69f-9c76-45f0-c997-95cc8917532d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=2, out_features=1, bias=True)\n",
            "  (1): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2-2. Loss Function & Optimizer\n",
        "\n",
        "loss_fn = nn.BCELoss() # for binary classification\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1)"
      ],
      "metadata": {
        "id": "7rkWftFGiErd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2-3. Train Loop\n",
        "\n",
        "for step in range(100):\n",
        "  output = model(X) # 모델이 계산한 값: forward propagation\n",
        "\n",
        "  loss = loss_fn(output, y) # cost 값 계산\n",
        "\n",
        "  optimizer.zero_grad() # gradient 0 초기화 -> backpropagation에서 계산된 기울기 값이 계속 누적되므로\n",
        "  loss.backward() # backpropagation weight가 loss에 얼마나 영향을 끼치는지 gradient 계산\n",
        "  optimizer.step() # weight update\n",
        "\n",
        "  if step % 10 == 0:\n",
        "    print(step, loss.item())"
      ],
      "metadata": {
        "id": "XU4BhZxvid5P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7b3d222-c58c-42e3-db08-a9b45f2441b0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.7722126245498657\n",
            "10 0.5102237462997437\n",
            "20 0.3892797827720642\n",
            "30 0.3181474506855011\n",
            "40 0.27081748843193054\n",
            "50 0.23658904433250427\n",
            "60 0.2104177474975586\n",
            "70 0.1896180510520935\n",
            "80 0.17261669039726257\n",
            "90 0.15842168033123016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Test Model"
      ],
      "metadata": {
        "id": "EF8HUZtmmcL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Test\n",
        "\n",
        "test_tensor = torch.FloatTensor([[1,1], [1,0], [0,1]])\n",
        "\n",
        "with torch.no_grad(): # 학습이 아닌 평가를 위한 것이므로 gradient 저장하지 않음\n",
        "  output = model(test_tensor)\n",
        "  print(output)  # 첫번째 [1,1]은 둘다 1이므로 0.5이상이 나와야함"
      ],
      "metadata": {
        "id": "dcz7olzqieY4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b25132b-29dc-4568-a591-d74a4635cdee"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.7822],\n",
            "        [0.1525],\n",
            "        [0.1524]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcVftJ2x2L02",
        "outputId": "06249def-896d-4a0d-ba59-814f46374a0a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<generator object Module.parameters at 0x7fa7f62d1620>\n"
          ]
        }
      ]
    }
  ]
}